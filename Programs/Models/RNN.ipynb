{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Loading scaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./10ms_50rows/\"\n",
    "\n",
    "S1_x = np.load(data_path + \"x_S1_scaled_XYZ_10ms_50lb.npy\").astype(np.float32)\n",
    "S1_y = np.load(data_path + \"y_S1_scaled_XYZ_10ms_50lb.npy\").astype(np.float32)\n",
    "\n",
    "S2_x = np.load(data_path + \"x_S2_scaled_XYZ_10ms_50lb.npy\").astype(np.float32)\n",
    "S2_y = np.load(data_path + \"y_S2_scaled_XYZ_10ms_50lb.npy\").astype(np.float32)\n",
    "\n",
    "S3_x = np.load(data_path + \"x_S3_scaled_XYZ_10ms_50lb.npy\").astype(np.float32)\n",
    "S3_y = np.load(data_path + \"y_S3_scaled_XYZ_10ms_50lb.npy\").astype(np.float32)\n",
    "\n",
    "C1_x = np.load(data_path + \"x_C1_scaled_XYZ_10ms_50lb.npy\").astype(np.float32)\n",
    "C1_y = np.load(data_path + \"y_C1_scaled_XYZ_10ms_50lb.npy\").astype(np.float32)\n",
    "\n",
    "C2_x = np.load(data_path + \"x_C2_scaled_XYZ_10ms_50lb.npy\").astype(np.float32)\n",
    "C2_y = np.load(data_path + \"y_C2_scaled_XYZ_10ms_50lb.npy\").astype(np.float32)\n",
    "\n",
    "C3_x = np.load(data_path + \"x_C3_scaled_XYZ_10ms_50lb.npy\").astype(np.float32)\n",
    "C3_y = np.load(data_path + \"y_C3_scaled_XYZ_10ms_50lb.npy\").astype(np.float32)\n",
    "\n",
    "# Set up training and testing dats\n",
    "# Train on S1 - C2, test on C3\n",
    "\n",
    "X_train = [S1_x, S2_x, S3_x, C1_x, C2_x]\n",
    "Y_train = [S1_y, S2_y, S3_y, C1_y, C2_y]\n",
    "\n",
    "X_train = np.vstack(X_train)\n",
    "Y_train = np.vstack(Y_train)\n",
    "\n",
    "X_test = C3_x\n",
    "Y_test = C3_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.65048856, -0.29491472,  0.04537556],\n",
       "        [ 0.6443313 , -0.30964693,  0.04340368],\n",
       "        [ 0.6350953 , -0.3219238 ,  0.06706619],\n",
       "        ...,\n",
       "        [-5.4313917 ,  7.0589256 ,  1.8733039 ],\n",
       "        [-5.346729  ,  7.7889895 ,  3.2023478 ],\n",
       "        [-5.132762  ,  8.492863  ,  3.6046102 ]],\n",
       "\n",
       "       [[ 1.1677023 ,  0.10285561,  0.03748805],\n",
       "        [ 1.1738597 ,  0.09548949,  0.02467087],\n",
       "        [ 1.1461518 ,  0.09385257,  0.01579743],\n",
       "        ...,\n",
       "        [-3.5133903 ,  4.6092825 ,  3.938843  ],\n",
       "        [-3.120862  ,  4.965311  ,  4.443643  ],\n",
       "        [-2.8576372 ,  5.409734  ,  5.0677414 ]],\n",
       "\n",
       "       [[ 0.9829832 ,  0.08484954,  0.34510058],\n",
       "        [ 0.9860618 ,  0.06193274,  0.31848025],\n",
       "        [ 1.0183877 ,  0.04310821,  0.32143807],\n",
       "        ...,\n",
       "        [ 6.3875594 ,  3.4699895 ,  8.507678  ],\n",
       "        [ 7.0002117 ,  3.2440953 ,  9.639534  ],\n",
       "        [ 7.515886  ,  2.9838257 , 10.531808  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.3277924 ,  0.31729147, -0.5708354 ],\n",
       "        [ 1.2862306 ,  0.34839284, -0.5826667 ],\n",
       "        [ 1.1461518 ,  0.40977713, -0.61914635],\n",
       "        ...,\n",
       "        [ 1.1307585 ,  0.39831874, -0.7137964 ],\n",
       "        [ 1.1107472 ,  0.41714326, -0.7394308 ],\n",
       "        [ 1.0630281 ,  0.42696476, -0.7640792 ]],\n",
       "\n",
       "       [[ 0.9429607 ,  0.5243612 , -0.72266984],\n",
       "        [ 0.9999157 ,  0.4891675 , -0.72266984],\n",
       "        [ 1.0522529 ,  0.45806614, -0.7177401 ],\n",
       "        ...,\n",
       "        [ 1.0507135 ,  0.4376047 , -0.735487  ],\n",
       "        [ 1.1153653 ,  0.41141406, -0.7492901 ],\n",
       "        [ 1.1369158 ,  0.39668182, -0.7502761 ]],\n",
       "\n",
       "       [[ 1.0307022 ,  0.44251543, -0.7719667 ],\n",
       "        [ 0.99375844,  0.45561075, -0.7798542 ],\n",
       "        [ 0.94142133,  0.48834905, -0.79957294],\n",
       "        ...,\n",
       "        [ 0.91987073,  0.5047182 , -0.8064745 ],\n",
       "        [ 0.8952415 ,  0.51535815, -0.830137  ],\n",
       "        [ 0.869073  ,  0.52927196, -0.84394014]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_lstm_cplx(X_train, Y_train, X_test, Y_test, time_steps, n_features, params):\n",
    "  # Build the Model\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(params['HL_1'], activation=tf.nn.relu, input_shape=(time_steps, n_features), return_sequences=True))\n",
    "  model.add(Dropout(params['drop1']))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(LSTM(params['HL_2'], activation=tf.nn.relu))\n",
    "  model.add(Dropout(params['drop2']))\n",
    "  model.add(BatchNormalization())\n",
    "#   model.add(Dense(10, activation = tf.nn.relu))\n",
    "  model.add(Dense(1, activation = tf.nn.sigmoid))\n",
    "    \n",
    "#   opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience = params['patience'], verbose=1)\n",
    "  model.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = 'nadam', metrics=['accuracy'])\n",
    "\n",
    "  # X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "  \n",
    "  # Implement class weights\n",
    "  cw = {\n",
    "      0: params['w0'],\n",
    "      1: params['w1']\n",
    "  }\n",
    "  model.summary()\n",
    "  # Train the model\n",
    "  history = model.fit(X_train, Y_train, epochs = params['epochs'], batch_size = params['batch'], class_weight=cw, callbacks=[callback])\n",
    "\n",
    "  # Predict on data\n",
    "  Y_pred = model.predict(X_test)\n",
    "  Y_pred = np.round(Y_pred, 0)\n",
    "  np.where(Y_pred == 1)\n",
    "  cm = confusion_matrix(Y_test, Y_pred)\n",
    "  print(\"CONFUSION MATRIX\")\n",
    "  print(cm)\n",
    "  print(\"CLASSIFICATION REPORT\")\n",
    "  print(classification_report(Y_test, Y_pred, target_names=[\"non_fall\", \"fall\"]))\n",
    "    \n",
    "  return history, cm, model\n",
    "\n",
    "\n",
    "params = { # With Feature Scaling\n",
    "    'drop1' : 0.4,\n",
    "    'drop2': 0.3,\n",
    "    'w1' : 750.,\n",
    "    'w0' : 1.,\n",
    "    'batch' : 7500,\n",
    "    'epochs' : 5,\n",
    "    'patience' : 5,\n",
    "    'HL_1' : 100,\n",
    "    'HL_2' : 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63099, 50, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57348, 50, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
